{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "from os.path import splitext, join, isfile, isdir, basename\n",
    "import argparse\n",
    "import numpy as np\n",
    "from scipy import misc, ndimage\n",
    "from keras import backend as K\n",
    "from keras.models import model_from_json, load_model\n",
    "import tensorflow as tf\n",
    "import layers_builder as layers\n",
    "from glob import glob\n",
    "from utils import utils\n",
    "from keras.utils.generic_utils import CustomObjectScope\n",
    "import cv2\n",
    "import math\n",
    "# -- Fix for macos, uncomment it\n",
    "# import matplotlib\n",
    "# matplotlib.use('TkAgg')\n",
    "# --\n",
    "import matplotlib.pyplot as plt\n",
    "import cvlib as cv\n",
    "from cvlib.object_detection import draw_bbox\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "\n",
    "from imageio import imread\n",
    "import json\n",
    "import time\n",
    "# These are the means for the ImageNet pretrained ResNet\n",
    "DATA_MEAN = np.array([[[123.68, 116.779, 103.939]]])  # RGB order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSPNet(object):\n",
    "    \"\"\"Pyramid Scene Parsing Network by Hengshuang Zhao et al 2017\"\"\"\n",
    "\n",
    "    def __init__(self, nb_classes, resnet_layers, input_shape, weights):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = nb_classes\n",
    "\n",
    "        json_path = join(\"weights\", \"keras\", weights + \".json\")\n",
    "        h5_path = join(\"weights\", \"keras\", weights + \".h5\")\n",
    "        if 'pspnet' in weights:\n",
    "            if os.path.isfile(json_path) and os.path.isfile(h5_path):\n",
    "                print(\"Keras model & weights found, loading...\")\n",
    "                with CustomObjectScope({'Interp': layers.Interp}):\n",
    "                    with open(json_path) as file_handle:\n",
    "                        self.model = model_from_json(file_handle.read())\n",
    "                self.model.load_weights(h5_path)\n",
    "            else:\n",
    "                print(\"No Keras model & weights found, import from npy weights.\")\n",
    "                self.model = layers.build_pspnet(nb_classes=nb_classes,\n",
    "                                                 resnet_layers=resnet_layers,\n",
    "                                                 input_shape=self.input_shape)\n",
    "                self.set_npy_weights(weights)\n",
    "        else:\n",
    "            print('Load pre-trained weights')\n",
    "            self.model = load_model(weights)\n",
    "\n",
    "    def predict(self, img, flip_evaluation=False):\n",
    "        \"\"\"\n",
    "        Predict segementation for an image.\n",
    "        Arguments:\n",
    "            img: must be rowsxcolsx3\n",
    "        \"\"\"\n",
    "\n",
    "        if img.shape[0:2] != self.input_shape:\n",
    "            print(\n",
    "                \"Input %s not fitting for network size %s, resizing. You may want to try sliding prediction for better results.\" % (\n",
    "                img.shape[0:2], self.input_shape))\n",
    "            img = misc.imresize(img, self.input_shape)\n",
    "\n",
    "        img = img - DATA_MEAN\n",
    "        img = img[:, :, ::-1]  # RGB => BGR\n",
    "        img = img.astype('float32')\n",
    "\n",
    "        probs = self.feed_forward(img, flip_evaluation)\n",
    "\n",
    "        return probs\n",
    "\n",
    "    def predict_sliding(self, full_img, flip_evaluation):\n",
    "        \"\"\"\n",
    "        Predict on tiles of exactly the network input shape.\n",
    "        This way nothing gets squeezed.\n",
    "        \"\"\"\n",
    "        tile_size = self.input_shape\n",
    "        classes = self.num_classes\n",
    "        overlap = 1 / 3\n",
    "\n",
    "        stride = math.ceil(tile_size[0] * (1 - overlap))\n",
    "        tile_rows = max(int(math.ceil((full_img.shape[0] - tile_size[0]) / stride) + 1), 1)  # strided convolution formula\n",
    "        tile_cols = max(int(math.ceil((full_img.shape[1] - tile_size[1]) / stride) + 1), 1)\n",
    "        print(\"Need %i x %i prediction tiles @ stride %i px\" % (tile_cols, tile_rows, stride))\n",
    "        full_probs = np.zeros((full_img.shape[0], full_img.shape[1], classes))\n",
    "        count_predictions = np.zeros((full_img.shape[0], full_img.shape[1], classes))\n",
    "        tile_counter = 0\n",
    "        for row in range(tile_rows):\n",
    "            for col in range(tile_cols):\n",
    "                x1 = int(col * stride)\n",
    "                y1 = int(row * stride)\n",
    "                x2 = min(x1 + tile_size[1], full_img.shape[1])\n",
    "                y2 = min(y1 + tile_size[0], full_img.shape[0])\n",
    "                x1 = max(int(x2 - tile_size[1]), 0)  # for portrait images the x1 underflows sometimes\n",
    "                y1 = max(int(y2 - tile_size[0]), 0)  # for very few rows y1 underflows\n",
    "\n",
    "                img = full_img[y1:y2, x1:x2]\n",
    "                padded_img = self.pad_image(img, tile_size)\n",
    "#                 plt.imshow(padded_img)\n",
    "#                 plt.show()\n",
    "                tile_counter += 1\n",
    "                print(\"Predicting tile %i\" % tile_counter)\n",
    "                padded_prediction = self.predict(padded_img, flip_evaluation)\n",
    "                prediction = padded_prediction[0:img.shape[0], 0:img.shape[1], :]\n",
    "                count_predictions[y1:y2, x1:x2] += 1\n",
    "                full_probs[y1:y2, x1:x2] += prediction  # accumulate the predictions also in the overlapping regions\n",
    "\n",
    "        # average the predictions in the overlapping regions\n",
    "        full_probs /= count_predictions\n",
    "        # visualize normalization Weights\n",
    "        # plt.imshow(np.mean(count_predictions, axis=2))\n",
    "        # plt.show()\n",
    "        return full_probs\n",
    "\n",
    "    @staticmethod\n",
    "    def pad_image(img, target_size):\n",
    "        \"\"\"Pad an image up to the target size.\"\"\"\n",
    "        rows_missing = target_size[0] - img.shape[0]\n",
    "        cols_missing = target_size[1] - img.shape[1]\n",
    "        padded_img = np.pad(img, ((0, rows_missing), (0, cols_missing), (0, 0)), 'constant')\n",
    "        return padded_img\n",
    "\n",
    "    def predict_multi_scale(self, img, flip_evaluation, sliding_evaluation, scales):\n",
    "        \"\"\"Predict an image by looking at it with different scales.\"\"\"\n",
    "\n",
    "        full_probs = np.zeros((img.shape[0], img.shape[1], self.num_classes))\n",
    "        h_ori, w_ori = img.shape[:2]\n",
    "\n",
    "        print(\"Started prediction...\")\n",
    "        for scale in scales:\n",
    "            print(\"Predicting image scaled by %f\" % scale)\n",
    "            scaled_img = misc.imresize(img, size=scale, interp=\"bilinear\")\n",
    "\n",
    "            if sliding_evaluation:\n",
    "                scaled_probs = self.predict_sliding(scaled_img, flip_evaluation)\n",
    "            else:\n",
    "                scaled_probs = self.predict(scaled_img, flip_evaluation)\n",
    "\n",
    "            # scale probs up to full size\n",
    "            # visualize_prediction(probs)\n",
    "            probs = cv2.resize(scaled_probs, (w_ori, h_ori))\n",
    "            full_probs += probs\n",
    "        full_probs /= len(scales)\n",
    "        print(\"Finished prediction...\")\n",
    "\n",
    "        return full_probs\n",
    "\n",
    "    def feed_forward(self, data, flip_evaluation=False):\n",
    "        assert data.shape == (self.input_shape[0], self.input_shape[1], 3)\n",
    "\n",
    "        if flip_evaluation:\n",
    "            print(\"Predict flipped\")\n",
    "            input_with_flipped = np.array(\n",
    "                [data, np.flip(data, axis=1)])\n",
    "            prediction_with_flipped = self.model.predict(input_with_flipped)\n",
    "            prediction = (prediction_with_flipped[\n",
    "                          0] + np.fliplr(prediction_with_flipped[1])) / 2.0\n",
    "        else:\n",
    "            prediction = self.model.predict(np.expand_dims(data, 0))[0]\n",
    "        return prediction\n",
    "\n",
    "    def set_npy_weights(self, weights_path):\n",
    "        npy_weights_path = join(\"weights\", \"npy\", weights_path + \".npy\")\n",
    "        json_path = join(\"weights\", \"keras\", weights_path + \".json\")\n",
    "        h5_path = join(\"weights\", \"keras\", weights_path + \".h5\")\n",
    "\n",
    "        print(\"Importing weights from %s\" % npy_weights_path)\n",
    "        weights = np.load(npy_weights_path, encoding='bytes').item()\n",
    "        for layer in self.model.layers:\n",
    "            print(layer.name)\n",
    "            if layer.name[:4] == 'conv' and layer.name[-2:] == 'bn':\n",
    "                mean = weights[layer.name.encode()][\n",
    "                    'mean'.encode()].reshape(-1)\n",
    "                variance = weights[layer.name.encode()][\n",
    "                    'variance'.encode()].reshape(-1)\n",
    "                scale = weights[layer.name.encode()][\n",
    "                    'scale'.encode()].reshape(-1)\n",
    "                offset = weights[layer.name.encode()][\n",
    "                    'offset'.encode()].reshape(-1)\n",
    "\n",
    "                self.model.get_layer(layer.name).set_weights(\n",
    "                    [scale, offset, mean, variance])\n",
    "\n",
    "            elif layer.name[:4] == 'conv' and not layer.name[-4:] == 'relu':\n",
    "                try:\n",
    "                    weight = weights[layer.name.encode()]['weights'.encode()]\n",
    "                    self.model.get_layer(layer.name).set_weights([weight])\n",
    "                except Exception as err:\n",
    "                    biases = weights[layer.name.encode()]['biases'.encode()]\n",
    "                    self.model.get_layer(layer.name).set_weights([weight,\n",
    "                                                                  biases])\n",
    "        print('Finished importing weights.')\n",
    "\n",
    "        print(\"Writing keras model & weights\")\n",
    "        json_string = self.model.to_json()\n",
    "        with open(json_path, 'w') as file_handle:\n",
    "            file_handle.write(json_string)\n",
    "        self.model.save_weights(h5_path)\n",
    "        print(\"Finished writing Keras model & weights\")\n",
    "\n",
    "\n",
    "class PSPNet50(PSPNet):\n",
    "    \"\"\"Build a PSPNet based on a 50-Layer ResNet.\"\"\"\n",
    "\n",
    "    def __init__(self, nb_classes, weights, input_shape):\n",
    "        PSPNet.__init__(self, nb_classes=nb_classes, resnet_layers=50,\n",
    "                        input_shape=input_shape, weights=weights)\n",
    "\n",
    "\n",
    "class PSPNet101(PSPNet):\n",
    "    \"\"\"Build a PSPNet based on a 101-Layer ResNet.\"\"\"\n",
    "\n",
    "    def __init__(self, nb_classes, weights, input_shape):\n",
    "        PSPNet.__init__(self, nb_classes=nb_classes, resnet_layers=101,\n",
    "                        input_shape=input_shape, weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_bbox(img_path):\n",
    "    img_nm = img_path.split('/')[-1]\n",
    "    \n",
    "    if img_nm in list(bbox_all.keys()):\n",
    "        bbox = bbox_all[img_nm]\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        msg = '>>> Error : No Bbox in this picture!'\n",
    "        print(msg)\n",
    "        raise Exception\n",
    "        pass\n",
    "    \n",
    "    return bbox    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ( 계정명 & 색상 추출 개수 ) 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_name = \"world_fashion_styles\"\n",
    "expected_num_col = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bbox( json ) 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_nm = account_name + '_bbox.json'\n",
    "\n",
    "with open(\"../Trip_Fashion_Recommend/data/\" + account_nm, 'r') as bbox_tmp:\n",
    "    bbox_all = json.load(bbox_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras model & weights found, loading...\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/pspnet_env/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:492: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/pspnet_env/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3630: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/pspnet_env/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:117: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/pspnet_env/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3458: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/pspnet_env/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3462: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /media/tatsch/DataSSD1/workspace/PSPNet-Keras-tensorflow/pspnet/layers_builder.py:20: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/pspnet_env/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3013: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/pspnet_env/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1208: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/pspnet_env/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1242: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/pspnet_env/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:158: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/pspnet_env/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:172: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Processing image 1 / 80\n",
      "image : world_fashion_styles_124_1299Likes.png\n",
      "Started prediction...\n",
      "Predicting image scaled by 1.000000\n",
      "Need 3 x 4 prediction tiles @ stride 316 px\n",
      "Predicting tile 1\n",
      "Predict flipped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pspnet_env/lib/python3.5/site-packages/ipykernel_launcher.py:108: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting tile 2\n",
      "Predict flipped\n",
      "Predicting tile 3\n",
      "Predict flipped\n",
      "Predicting tile 4\n",
      "Predict flipped\n",
      "Predicting tile 5\n",
      "Predict flipped\n",
      "Predicting tile 6\n",
      "Predict flipped\n",
      "Predicting tile 7\n",
      "Predict flipped\n",
      "Predicting tile 8\n",
      "Predict flipped\n",
      "Predicting tile 9\n",
      "Predict flipped\n",
      "Predicting tile 10\n",
      "Predict flipped\n",
      "Predicting tile 11\n",
      "Predict flipped\n",
      "Predicting tile 12\n",
      "Predict flipped\n",
      "Finished prediction...\n",
      "Processing image 2 / 80\n",
      "image : world_fashion_styles_002_2678Likes.png\n",
      "Started prediction...\n",
      "Predicting image scaled by 1.000000\n",
      "Need 3 x 4 prediction tiles @ stride 316 px\n",
      "Predicting tile 1\n",
      "Predict flipped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pspnet_env/lib/python3.5/site-packages/ipykernel_launcher.py:108: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting tile 2\n",
      "Predict flipped\n",
      "Predicting tile 3\n",
      "Predict flipped\n",
      "Predicting tile 4\n",
      "Predict flipped\n",
      "Predicting tile 5\n",
      "Predict flipped\n",
      "Predicting tile 6\n",
      "Predict flipped\n",
      "Predicting tile 7\n",
      "Predict flipped\n",
      "Predicting tile 8\n",
      "Predict flipped\n",
      "Predicting tile 9\n",
      "Predict flipped\n",
      "Predicting tile 10\n",
      "Predict flipped\n",
      "Predicting tile 11\n",
      "Predict flipped\n",
      "Predicting tile 12\n",
      "Predict flipped\n",
      "Finished prediction...\n",
      "time elapsed while training : 192.39666199684143\n"
     ]
    }
   ],
   "source": [
    "img_loc = glob(\"../Trip_Fashion_Recommend/data/pictures_{}/*\".format(account_name))\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4 \n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "K.set_session(sess)\n",
    "\n",
    "model = \"pspnet50_ade20k\"\n",
    "\n",
    "with sess.as_default():\n",
    "    pspnet = PSPNet50(nb_classes=150, input_shape=(473, 473),\n",
    "                              weights=model)\n",
    "    lb_dict = {}\n",
    "    begin_time = time.time()\n",
    "    for i, img_path in enumerate(img_loc[:2]):\n",
    "            print(\"Processing image {} / {}\".format(i + 1, len(img_loc)))\n",
    "            print(\"image : {}\".format(img_path.split('/')[-1]))\n",
    "            img = imread(img_path, pilmode=\"RGB\")\n",
    "\n",
    "            probs = pspnet.predict_multi_scale(img, flip_evaluation=True, sliding_evaluation = True,  scales=[1.0])\n",
    "\n",
    "            cm = np.argmax(probs, axis=2)\n",
    "            \n",
    "\n",
    "            try:\n",
    "                bbox = loading_bbox(img_path)\n",
    "                \n",
    "                # Segmentation 처리가 된 이미지에서 bbox에 해당하는 부분만 cropping\n",
    "                xx = cm[bbox[0][1]:bbox[0][3], bbox[0][0]:bbox[0][2]]\n",
    "                unique_elements, counts_elements = np.unique(xx, return_counts=True)\n",
    "                unique_elements[np.where(counts_elements==max(counts_elements))][0]\n",
    "                a,b = np.where(xx == unique_elements[np.where(counts_elements==max(counts_elements))][0])\n",
    "                img_crop = img[bbox[0][1]:bbox[0][3], bbox[0][0]:bbox[0][2]]\n",
    "#                 plt.imshow(img_crop)\n",
    "#                 plt.show()\n",
    "\n",
    "                # Extract labels & cluster_centers_ by using KMeans\n",
    "                clt = KMeans(n_clusters = expected_num_col)\n",
    "                clt.fit(img_crop[a,b])\n",
    "                lb_counts = Counter(clt.labels_)\n",
    "\n",
    "                tmp_dict = {}\n",
    "                filename = img_path.split('/')[-1]\n",
    "                for idx, pixel in enumerate(clt.cluster_centers_):\n",
    "                    tmp_dict[ str(lb_counts[idx]) ] = pixel.tolist()  # { frequency of label : color of label } mapping\n",
    "                    lb_dict[filename] = tmp_dict\n",
    "                    pass\n",
    "                pass\n",
    "            \n",
    "            except Exception as e:\n",
    "                msg = '>>> Error in {}th file : {}\\n'.format(i+1, img_path)\n",
    "                print(msg)\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    # json 파일에 lb_count , clt.cluster_centers_ 저장\n",
    "    save_path = \"../data/{}.json\".format(account_name)\n",
    "    with open(save_path, 'w') as json_file:\n",
    "        json.dump(lb_dict, json_file)\n",
    "\n",
    "end_time = time.time()\n",
    "  \n",
    "print('time elapsed while training : {}'.format(end_time - begin_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
